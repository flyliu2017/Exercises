XGB:
{'max_depth': 30, 'learning_rate': 0.1, 'n_estimators': 100}
====================
score:0.955
              precision    recall  f1-score   support

           0    0.99600   0.99700   0.99650      1000
           1    0.98599   0.98500   0.98549      1000
           2    0.93524   0.88100   0.90731      1000
           3    0.99798   0.98600   0.99195      1000
           4    0.95530   0.91900   0.93680      1000
           5    0.95562   0.96900   0.96226      1000
           6    0.92004   0.95500   0.93719      1000
           7    0.97600   0.97600   0.97600      1000
           8    0.95127   0.97600   0.96347      1000
           9    0.96311   0.99200   0.97734      1000

   micro avg    0.96360   0.96360   0.96360     10000
   macro avg    0.96365   0.96360   0.96343     10000
weighted avg    0.96365   0.96360   0.96343     10000

RF:
{'max_features': 600}
====================
score:0.9278
              precision    recall  f1-score   support

           0    0.99799   0.99400   0.99599      1000
           1    0.97413   0.97900   0.97656      1000
           2    0.89636   0.78700   0.83813      1000
           3    0.98900   0.98900   0.98900      1000
           4    0.92013   0.86400   0.89118      1000
           5    0.96092   0.95900   0.95996      1000
           6    0.84029   0.92600   0.88107      1000
           7    0.96878   0.96200   0.96538      1000
           8    0.94055   0.96500   0.95262      1000
           9    0.93603   0.99500   0.96461      1000

   micro avg    0.94200   0.94200   0.94200     10000
   macro avg    0.94242   0.94200   0.94145     10000
weighted avg    0.94242   0.94200   0.94145     10000

{'max_features': 800}
====================
score:0.931
              precision    recall  f1-score   support

           0    0.99500   0.99500   0.99500      1000
           1    0.97892   0.97500   0.97695      1000
           2    0.89853   0.79700   0.84473      1000
           3    0.99198   0.98900   0.99049      1000
           4    0.91429   0.86400   0.88843      1000
           5    0.95069   0.96400   0.95730      1000
           6    0.83514   0.92200   0.87643      1000
           7    0.97454   0.95700   0.96569      1000
           8    0.94406   0.96200   0.95295      1000
           9    0.94223   0.99500   0.96790      1000

   micro avg    0.94200   0.94200   0.94200     10000
   macro avg    0.94254   0.94200   0.94159     10000
weighted avg    0.94254   0.94200   0.94159     10000

{'max_features': 1000}
====================
score:0.9202
              precision    recall  f1-score   support

           0    0.99400   0.99400   0.99400      1000
           1    0.97392   0.97100   0.97246      1000
           2    0.88380   0.79100   0.83483      1000
           3    0.99497   0.99000   0.99248      1000
           4    0.90698   0.85800   0.88181      1000
           5    0.94778   0.96200   0.95484      1000
           6    0.84686   0.91800   0.88100      1000
           7    0.96181   0.95700   0.95940      1000
           8    0.94857   0.95900   0.95375      1000
           9    0.93691   0.99500   0.96508      1000

   micro avg    0.93950   0.93950   0.93950     10000
   macro avg    0.93956   0.93950   0.93896     10000
weighted avg    0.93956   0.93950   0.93896     10000

gbdt:

{'max_features': 300, 'max_depth': 20, 'n_estimators': 500}
====================
Tue Nov 20 11:31:25 2018
      Iter       Train Loss   Remaining Time
         1       81502.2718           54.57m
         2       65234.6136           59.40m
         3       54097.0463           62.04m
         4       45938.9018           63.74m
         5       39433.3416           64.74m
         6       34156.3133           65.45m
         7       29752.5924           65.90m
         8       26077.7727           66.66m
         9       23027.2919           67.03m
        10       20447.2304           67.12m
        20        7162.1619           66.06m
        30        3491.4628           60.69m
        40        2151.8669           55.84m
        50        1561.2816           51.76m
        60        1198.2961           48.56m
        70         951.9549           45.88m
        80         774.6522           43.48m
        90         637.0712           41.33m
       100         537.1009           39.54m
       200         120.5756           25.49m
       300          34.3387           15.62m
       400          15.3563            7.03m
       500          12.8179            0.00s
score:0.9696
              precision    recall  f1-score   support

           0    0.99900   0.99900   0.99900      1000
           1    0.98799   0.98700   0.98749      1000
           2    0.93207   0.93300   0.93253      1000
           3    0.98792   0.98100   0.98445      1000
           4    0.96907   0.94000   0.95431      1000
           5    0.98185   0.97400   0.97791      1000
           6    0.95054   0.96100   0.95574      1000
           7    0.98981   0.97100   0.98031      1000
           8    0.95623   0.98300   0.96943      1000
           9    0.97171   0.99600   0.98370      1000

   micro avg    0.97250   0.97250   0.97250     10000
   macro avg    0.97262   0.97250   0.97249     10000
weighted avg    0.97262   0.97250   0.97249     10000

time cost:30.82 min

lr:

Mon Nov 19 21:29:17 2018
score:0.9544
              precision    recall  f1-score   support

           0    0.99599   0.99400   0.99499      1000
           1    0.98693   0.98200   0.98446      1000
           2    0.92873   0.83400   0.87882      1000
           3    0.87973   0.90700   0.89316      1000
           4    0.96751   0.92300   0.94473      1000
           5    0.96912   0.97300   0.97106      1000
           6    0.91992   0.96500   0.94192      1000
           7    0.99082   0.97100   0.98081      1000
           8    0.93911   0.98700   0.96246      1000
           9    0.95192   0.99000   0.97059      1000

   micro avg    0.95260   0.95260   0.95260     10000
   macro avg    0.95298   0.95260   0.95230     10000
weighted avg    0.95298   0.95260   0.95230     10000

time cost:0.16 min

stacking:(meta_clf=LR)

score:0.9754
              precision    recall  f1-score   support

           0    0.99900   0.99800   0.99850      1000
           1    0.99196   0.98700   0.98947      1000
           2    0.94990   0.92900   0.93933      1000
           3    0.98897   0.98600   0.98748      1000
           4    0.97029   0.94700   0.95850      1000
           5    0.97700   0.97700   0.97700      1000
           6    0.95682   0.97500   0.96582      1000
           7    0.98590   0.97900   0.98244      1000
           8    0.95902   0.98300   0.97086      1000
           9    0.97741   0.99500   0.98612      1000

   micro avg    0.97560   0.97560   0.97560     10000
   macro avg    0.97563   0.97560   0.97555     10000
weighted avg    0.97563   0.97560   0.97555     10000

time cost:57.66 min
python -m nmt.nmt     --src=vi --tgt=en     --vocab_prefix=/root/PycharmProjects/tf/exercises/NLP/NMT/nmt-master/data/vocab      --train_prefix=/root/PycharmProjects/tf/exercises/NLP/NMT/nmt-master/data/train     --dev_prefix=/root/PycharmProjects/tf/exercises/NLP/NMT/nmt-master/data/tst2012      --test_prefix=/root/PycharmProjects/tf/exercises/NLP/NMT/nmt-master/data/tst2013     --out_dir=/root/PycharmProjects/nmt_model     --num_train_steps=50000     --steps_per_stats=100     --num_layers=2     --num_units=128     --dropout=0.2     --metrics=bleu --override_loaded_hparams=True


python -m nmt.nmt     --src=vi --tgt=en     \
        --vocab_prefix=/root/PycharmProjects/tf/exercises/NLP/NMT/nmt-master/data/vocab      \
        --train_prefix=/root/PycharmProjects/tf/exercises/NLP/NMT/nmt-master/data/train      \
        --dev_prefix=/root/PycharmProjects/tf/exercises/NLP/NMT/nmt-master/data/tst2012      \
        --test_prefix=/root/PycharmProjects/tf/exercises/NLP/NMT/nmt-master/data/tst2013     \
        --out_dir=/root/PycharmProjects/nmt_model     \
        --num_train_steps=50000     \
        --steps_per_stats=100     \
        --num_layers=2     \
        --num_units=128     \
        --dropout=0.2     \
        --metrics=bleu \
        --override_loaded_hparams=False \
        --model_device_fn=tf.train.replica_device_setter(ps_tasks)


python3 -m nmt.nmt     --src=vi --tgt=en     --vocab_prefix=/home/paidev/liuchang/NMT/nmt-master/data/vocab      --train_prefix=/home/paidev/liuchang/NMT/nmt-master/data/train     --dev_prefix=/home/paidev/liuchang/NMT/nmt-master/data/tst2012      --test_prefix=/home/paidev/liuchang/NMT/nmt-master/data/tst2013     --out_dir=/root/PycharmProjects/nmt_model     --num_train_steps=50000     --steps_per_stats=100     --num_layers=8     --num_units=128     --dropout=0.2     --metrics=bleu --override_loaded_hparams=True --num_gpus=8

python tf_cnn_benchmarks.py --data_format=NCHW --batch_size=256 \
--model=resnet50 --optimizer=momentum --variable_update=replicated \
--nodistortions --gradient_repacking=8 --num_gpus=8 \
--num_epochs=90 --weight_decay=1e-4 --data_dir=${DATA_DIR} --use_fp16 \
--train_dir=${CKPT_DIR}

python -m nmt.nmt     --src=vi --tgt=en     ^
        --vocab_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data\vocab      ^
        --train_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data\train      ^
        --dev_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data\tst2012      ^
        --test_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data\tst2013     ^
        --out_dir=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\nmt_model_512     ^
        --steps_per_stats=100     ^
        --num_layers=2     ^
        --num_units=384     ^
        --dropout=0.2     ^
        --metrics=bleu ^
        --attention="luong" ^
        --override_loaded_hparams=True ^
        --learning_rate=1 ^
        --decay_scheme="luong5" ^
        --num_train_steps=100000

python -m nmt.nmt     --src=en --tgt=zh     ^
        --vocab_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_edu\vocab20000      ^
        --train_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_edu\train      ^
        --dev_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_edu\dev      ^
        --test_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_edu\test     ^
        --out_dir=E:\nmt_save\nmt_model_en_zh_20000     ^
        --steps_per_stats=100     ^
        --num_layers=2     ^
        --num_units=256     ^
        --dropout=0.2     ^
        --metrics=bleu ^
        --attention="luong" ^
        --override_loaded_hparams=True ^
        --learning_rate=0.05 ^
        --decay_scheme="luong5" ^
        --num_train_steps=60000

python -m nmt.nmt     --src=en --tgt=zh     ^
        --vocab_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_Spoken\vocab_Spoken_30000      ^
        --train_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_Spoken\Spoken_train      ^
        --dev_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_Spoken\Spoken_dev      ^
        --test_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_Spoken\Spoken_test     ^
        --out_dir=E:\nmt_save\nmt_model_en_zh_Spoken_30000     ^
        --steps_per_stats=200     ^
        --num_layers=2     ^
        --num_units=256     ^
        --dropout=0.2     ^
        --metrics=bleu ^
        --attention="normed_bahdanau" ^
        --attention_architecture="gnmt" ^
        --encoder_type="gnmt" ^
        --subword_option=bpe ^
        --override_loaded_hparams=True ^
        --learning_rate=0.05 ^
        --decay_scheme="luong5" ^
        --steps_per_external_eval=200 ^
        --batch_size=96 ^
        --num_train_steps=30000


python -m nmt.nmt --src=en --tgt=zh --vocab_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_Spoken\vocab_Spoken_30000 --train_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_Spoken\Spoken_train --dev_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_Spoken\Spoken_dev --test_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_Spoken\Spoken_test --out_dir=E:\nmt_save\nmt_model_en_zh_Spoken_30000 --steps_per_stats=200 --num_layers=2 --num_units=256 --dropout=0.2 --metrics=bleu --attention="normed_bahdanau" --attention_architecture="gnmt" --encoder_type="gnmt" --subword_option=bpe --override_loaded_hparams=True --learning_rate=0.004 --decay_scheme="luong5" --steps_per_external_eval=10000 --batch_size=64 --num_train_steps=120000

python -m nmt.nmt --src=en --tgt=zh --vocab_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_datum1~2\vocab_datum1~2_30000 --train_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_datum1~2\datum1~2_train --dev_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_datum1~2\datum1~2_dev --test_prefix=E:\pyProject\TensorflowExercises\NLP\nmt\nmt\data_datum1~2\datum1~2_test --out_dir=E:\nmt_save\nmt_model_en_zh_datum1~2_30000 --steps_per_stats=200 --num_layers=2 --num_units=256 --dropout=0.2 --metrics=bleu --attention="normed_bahdanau" --attention_architecture="gnmt" --encoder_type="gnmt" --subword_option=bpe --override_loaded_hparams=True --learning_rate=0.5 --decay_scheme="luong5" --steps_per_external_eval=10000 --batch_size=64 --num_train_steps=50000